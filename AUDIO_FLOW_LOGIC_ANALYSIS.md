# Audio Flow Logic Analysis: User Input to TTS Output

## üéØ **Overview**

This document provides a comprehensive analysis of the complete audio flow logic from user input to TTS output in the Dharsan VoiceAgent frontend. The system uses a sophisticated multi-protocol approach combining WebRTC (WHIP) for audio transport and WebSocket for real-time communication.

## üîÑ **Complete Audio Flow Architecture**

### **High-Level Flow**
```
User Speech ‚Üí Microphone ‚Üí AudioWorklet ‚Üí WebRTC (WHIP) ‚Üí Media Server ‚Üí Kafka ‚Üí Orchestrator ‚Üí STT ‚Üí LLM ‚Üí TTS ‚Üí WebSocket ‚Üí Frontend ‚Üí Audio Playback
```

### **Frontend Components Flow**
```
User Interface ‚Üí useVoiceAgentWHIP Hook ‚Üí Audio Processing ‚Üí WebRTC Connection ‚Üí WebSocket Communication ‚Üí Audio Playback
```

## üé§ **1. Audio Input Capture (User Speech)**

### **Microphone Access**
```typescript
// Location: useVoiceAgentWHIP_fixed.ts - connect() function
const stream = await navigator.mediaDevices.getUserMedia({ 
  audio: {
    sampleRate: 48000,        // High-quality audio sampling
    channelCount: 1,          // Mono audio for efficiency
    echoCancellation: true,   // Reduce echo
    noiseSuppression: true,   // Reduce background noise
    autoGainControl: true     // Automatic volume adjustment
  } 
});
```

### **Audio Context Setup**
```typescript
// Create Web Audio API context for processing
const audioContext = new AudioContext();
const source = audioContext.createMediaStreamSource(stream);
const analyser = audioContext.createAnalyser();

// Configure analyzer for voice activity detection
analyser.fftSize = 256;  // Frequency analysis resolution
const bufferLength = analyser.frequencyBinCount;
const dataArray = new Uint8Array(bufferLength);
```

### **Voice Activity Detection**
```typescript
// Real-time voice activity monitoring
const detectVoiceActivity = () => {
  analyser.getByteFrequencyData(dataArray);
  const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
  
  // Update audio level (0-100%)
  const audioLevel = Math.min(100, Math.max(0, (average / 255) * 100));
  setState(prev => ({ ...prev, audioLevel }));
  
  // Log significant voice activity
  if (average > 50) {
    console.log('üéµ [AUDIO] Strong voice detected - Level:', audioLevel.toFixed(1) + '%');
  }
};
```

## üîß **2. Audio Processing (AudioWorklet)**

### **AudioWorklet Processor**
**Location**: `public/audio-processor.js`

```javascript
class AudioProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.audioLevel = 0;
    this.smoothingFactor = 0.8;  // Smooth audio level changes
  }

  process(inputs, outputs, parameters) {
    const input = inputs[0];
    const output = outputs[0];

    if (input && input.length > 0) {
      const inputChannel = input[0];
      
      // Calculate audio level (RMS - Root Mean Square)
      let sum = 0;
      for (let i = 0; i < inputChannel.length; i++) {
        sum += inputChannel[i] * inputChannel[i];
      }
      const rms = Math.sqrt(sum / inputChannel.length);
      
      // Scale RMS for better visualization
      const scaledLevel = Math.min(1, rms * 5);
      
      // Smooth the audio level to reduce jitter
      this.audioLevel = this.smoothingFactor * this.audioLevel + 
                       (1 - this.smoothingFactor) * scaledLevel;
      
      // Send audio level to main thread
      this.port.postMessage({
        type: 'audioLevel',
        level: this.audioLevel
      });

      // Echo audio for testing (pass-through)
      if (output && output.length > 0) {
        const outputChannel = output[0];
        for (let i = 0; i < inputChannel.length; i++) {
          outputChannel[i] = inputChannel[i];
        }
      }
    }

    return true;  // Keep processor alive
  }
}
```

### **AudioWorklet Integration**
```typescript
// Load and register AudioWorklet
await audioContext.audioWorklet.addModule('/audio-processor.js');
const processor = new AudioWorkletNode(audioContext, 'audio-processor');

// Handle audio level messages from worklet
processor.port.onmessage = (event) => {
  if (event.data.type === 'audioLevel') {
    const percentage = Math.round(event.data.level * 100);
    setAudioLevel(percentage);
  }
};

// Connect audio chain
source.connect(processor);
processor.connect(audioContext.destination);
```

## üåê **3. WebRTC Connection (WHIP Protocol)**

### **Peer Connection Setup**
```typescript
// Create RTCPeerConnection with ICE servers
const peerConnection = new RTCPeerConnection({
  iceServers: [
    { urls: 'stun:stun.l.google.com:19302' },
    { urls: 'stun:stun1.l.google.com:19302' },
    { urls: 'stun:stun.cloudflare.com:3478' }
  ]
});

// Add local audio track to peer connection
stream.getTracks().forEach(track => {
  peerConnection.addTrack(track, stream);
});
```

### **WHIP Protocol Implementation**
```typescript
// Create SDP offer
const offer = await peerConnection.createOffer({
  offerToReceiveAudio: true,  // Expect audio from server
  offerToReceiveVideo: false  // No video needed
});

// Set local description
await peerConnection.setLocalDescription(offer);

// Send WHIP request to media server
const response = await fetch(`${mediaServerUrl}/whip`, {
  method: 'POST',
  headers: {
    'Content-Type': 'application/sdp',
    'X-Session-ID': sessionId
  },
  body: offer.sdp
});

// Handle SDP answer
const answerSdp = await response.text();
const answer = new RTCSessionDescription({ 
  type: 'answer', 
  sdp: answerSdp 
});
await peerConnection.setRemoteDescription(answer);
```

### **Connection State Monitoring**
```typescript
// Monitor connection quality
peerConnection.onconnectionstatechange = () => {
  switch (peerConnection.connectionState) {
    case 'connected':
      setState(prev => ({ ...prev, connectionStatus: 'connected' }));
      break;
    case 'disconnected':
    case 'failed':
      setState(prev => ({ ...prev, connectionStatus: 'error' }));
      break;
  }
};

// Monitor ICE connection state
peerConnection.oniceconnectionstatechange = () => {
  const iceState = peerConnection.iceConnectionState;
  const connectionState = peerConnection.connectionState;
  
  let connectionQuality: 'excellent' | 'good' | 'poor' | 'unknown' = 'unknown';
  
  if (iceState === 'connected' || iceState === 'completed') {
    if (connectionState === 'connected') {
      connectionQuality = 'excellent';
    } else {
      connectionQuality = 'good';
    }
  } else if (iceState === 'checking') {
    connectionQuality = 'good';
  } else {
    connectionQuality = 'poor';
  }
  
  setState(prev => ({ ...prev, connectionQuality }));
};
```

## üì° **4. WebSocket Communication (Orchestrator)**

### **WebSocket Connection Setup**
```typescript
// Connect to orchestrator WebSocket
const { orchestratorWsUrl } = getServiceUrls();
const websocket = new WebSocket(orchestratorWsUrl);

websocket.onopen = () => {
  // Send session info to orchestrator
  const sessionInfo = {
    type: 'session_info',
    session_id: sessionId,
    version: 'phase2'
  };
  websocket.send(JSON.stringify(sessionInfo));
  
  // Start heartbeat for connection monitoring
  startHeartbeat();
};
```

### **Message Handling**
```typescript
websocket.onmessage = (event) => {
  const data = JSON.parse(event.data);

  switch (data.type) {
    case 'final_transcript':
      // Handle user speech transcript
      console.log('üé§ [STT] User said:', data.transcript);
      setState(prev => ({
        ...prev,
        transcript: data.transcript,
        conversationHistory: [...prev.conversationHistory, {
          id: `user_${Date.now()}_${Math.random()}`,
          type: 'user',
          text: data.transcript,
          timestamp: new Date()
        }],
        isProcessing: true
      }));
      break;

    case 'ai_response':
      // Handle AI generated response
      console.log('ü§ñ [AI] Response:', data.response);
      setState(prev => ({
        ...prev,
        aiResponse: data.response,
        conversationHistory: [...prev.conversationHistory, {
          id: `ai_${Date.now()}_${Math.random()}`,
          type: 'ai',
          text: data.response,
          timestamp: new Date()
        }],
        isProcessing: false
      }));
      break;

    case 'processing_start':
      setState(prev => ({ ...prev, isProcessing: true }));
      break;

    case 'processing_complete':
      setState(prev => ({ ...prev, isProcessing: false }));
      break;

    case 'error':
      setState(prev => ({ ...prev, error: data.error, isProcessing: false }));
      break;
  }
};
```

## üîä **5. Audio Playback (TTS Output)**

### **Audio Reception and Processing**
```typescript
// Handle incoming audio data (TTS output)
if (event.data instanceof Blob) {
  console.log(`üîç TTS PINPOINT: Received Blob audio chunk, size: ${event.data.size} bytes`);
  
  // Convert Blob to ArrayBuffer for Web Audio API
  event.data.arrayBuffer().then(async (arrayBuffer) => {
    // Check for completion signal (empty blob)
    if (arrayBuffer.byteLength === 0) {
      console.log('üîç TTS PINPOINT: Received completion signal - audio stream ended');
      if (!isAudioPlayingRef.current && audioChunksRef.current.length > 0) {
        playContinuousAudio();
      }
      return;
    }
    
    // Add audio chunk to queue
    audioChunksRef.current.push(arrayBuffer);
    
    // Start playback if we have enough chunks
    if (!isAudioPlayingRef.current && audioChunksRef.current.length >= 3) {
      playContinuousAudio();
    }
  });
}
```

### **Continuous Audio Playback**
```typescript
const playContinuousAudio = useCallback(async () => {
  if (isAudioPlayingRef.current || audioChunksRef.current.length === 0) {
    return;
  }

  isAudioPlayingRef.current = true;
  setListeningState('speaking');

  try {
    // Resume audio context if suspended
    if (audioContextRef.current && audioContextRef.current.state === 'suspended') {
      await audioContextRef.current.resume();
    }

    // Wait for sufficient chunks for smooth playback
    if (audioChunksRef.current.length < 2) {
      setTimeout(() => playContinuousAudio(), 100);
      return;
    }

    // Take all available chunks for seamless playback
    const allChunks = [...audioChunksRef.current];
    audioChunksRef.current = [];
    
    // Create single blob from all chunks
    const audioBlob = new Blob(allChunks, { type: 'audio/mp3' });
    const audioUrl = URL.createObjectURL(audioBlob);
    
    const audio = new Audio(audioUrl);
    audio.preload = 'auto';
    audio.playbackRate = 1.0;
    audio.volume = 0.9;
    
    // Connect to Web Audio API for enhanced quality
    if (audioContextRef.current && audioContextRef.current.state === 'running') {
      const source = audioContextRef.current.createMediaElementSource(audio);
      const gainNode = audioContextRef.current.createGain();
      gainNode.gain.setValueAtTime(0.9, audioContextRef.current.currentTime);
      
      source.connect(gainNode);
      gainNode.connect(audioContextRef.current.destination);
    }
    
    // Audio event handlers
    audio.oncanplay = () => {
      audio.play().catch(error => {
        console.error('Audio playback failed:', error);
      });
    };
    
    audio.onended = () => {
      isAudioPlayingRef.current = false;
      setListeningState('idle');
      URL.revokeObjectURL(audioUrl);
    };
    
    audio.onerror = (error) => {
      console.error('Audio playback error:', error);
      isAudioPlayingRef.current = false;
      setListeningState('idle');
    };
    
  } catch (error) {
    console.error('Audio playback setup failed:', error);
    isAudioPlayingRef.current = false;
    setListeningState('idle');
  }
}, []);
```

## üéõÔ∏è **6. State Management and UI Updates**

### **Voice Agent State**
```typescript
interface VoiceAgentState {
  isConnected: boolean;
  isConnecting: boolean;
  connectionStatus: 'disconnected' | 'connecting' | 'connected' | 'error';
  error: string | null;
  transcript: string;
  isListening: boolean;
  sessionId: string | null;
  version: string;
  
  // Phase 2 specific states
  aiResponse: string | null;
  isProcessing: boolean;
  conversationHistory: Array<{
    id: string;
    type: 'user' | 'ai';
    text: string;
    timestamp: Date;
  }>;
  audioLevel: number;
  connectionQuality: 'excellent' | 'good' | 'poor' | 'unknown';
}
```

### **Real-time UI Updates**
```typescript
// Audio level visualization
const audioLevel = Math.min(100, Math.max(0, (average / 255) * 100));
setState(prev => ({ ...prev, audioLevel }));

// Connection quality monitoring
let connectionQuality: 'excellent' | 'good' | 'poor' | 'unknown' = 'unknown';
if (peerConnectionRef.current) {
  const iceState = peerConnectionRef.current.iceConnectionState;
  const connectionState = peerConnectionRef.current.connectionState;
  
  if (iceState === 'connected' || iceState === 'completed') {
    if (connectionState === 'connected') {
      connectionQuality = audioLevel > 50 ? 'excellent' : audioLevel > 20 ? 'good' : 'poor';
    } else {
      connectionQuality = 'poor';
    }
  } else if (iceState === 'checking') {
    connectionQuality = 'good';
  } else {
    connectionQuality = 'poor';
  }
}

setState(prev => ({ ...prev, connectionQuality }));
```

## üîÑ **7. Complete Data Flow Timeline**

### **User Speech to TTS Output Timeline**
```
1. User speaks ‚Üí Microphone captures audio
   ‚Üì (0-10ms)
2. AudioWorklet processes audio ‚Üí Voice activity detection
   ‚Üì (10-20ms)
3. WebRTC (WHIP) sends audio to Media Server
   ‚Üì (20-50ms)
4. Media Server publishes audio to Kafka
   ‚Üì (50-100ms)
5. Orchestrator consumes audio from Kafka
   ‚Üì (100-200ms)
6. STT Service processes audio ‚Üí Generates transcript
   ‚Üì (200-500ms)
7. LLM Service generates AI response
   ‚Üì (500-2000ms)
8. TTS Service generates audio response
   ‚Üì (2000-3000ms)
9. Orchestrator sends audio via WebSocket to Frontend
   ‚Üì (3000-3100ms)
10. Frontend receives audio chunks ‚Üí Audio playback
    ‚Üì (3100-3500ms)
11. User hears AI response
```

## üéØ **8. Key Technical Features**

### **Audio Quality Optimization**
- **High Sample Rate**: 48kHz for professional audio quality
- **Mono Channel**: Single channel for efficiency and compatibility
- **Echo Cancellation**: Built-in echo cancellation for better audio
- **Noise Suppression**: Automatic background noise reduction
- **Auto Gain Control**: Automatic volume adjustment

### **Latency Optimization**
- **WebRTC Transport**: UDP-based transport for ultra-low latency
- **Audio Buffering**: Minimal buffering for real-time conversation
- **Chunked Playback**: Continuous audio streaming without gaps
- **Connection Monitoring**: Real-time connection quality assessment

### **Error Handling and Recovery**
- **Connection Timeouts**: Automatic timeout detection and recovery
- **Audio Context Management**: Proper audio context lifecycle management
- **WebSocket Reconnection**: Automatic WebSocket reconnection logic
- **Error State Management**: Comprehensive error state tracking

### **Performance Monitoring**
- **Audio Level Monitoring**: Real-time audio input level tracking
- **Connection Quality**: ICE connection state monitoring
- **Processing States**: AI pipeline state tracking
- **Latency Measurement**: End-to-end latency calculation

## üèÜ **Summary**

The audio flow logic in the Dharsan VoiceAgent frontend represents a sophisticated, production-ready implementation that combines:

- **Advanced Audio Processing**: Web Audio API + AudioWorklet for real-time processing
- **Multi-Protocol Communication**: WebRTC (WHIP) + WebSocket for optimal performance
- **Real-time State Management**: Comprehensive state tracking and UI updates
- **Quality Optimization**: High-quality audio capture and playback
- **Error Resilience**: Robust error handling and recovery mechanisms
- **Performance Monitoring**: Real-time performance and quality metrics

The system successfully delivers a seamless, low-latency conversational AI experience with professional-grade audio quality and reliable real-time communication. 