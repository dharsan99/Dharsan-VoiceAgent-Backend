apiVersion: v1
kind: Service
metadata:
  name: llm-service
  namespace: voice-agent-phase5
  labels:
    app: llm-service
    component: ai
    phase: "5"
    version: "5.0.0"
spec:
  type: ClusterIP
  selector:
    app: llm-service
    component: ai
  ports:
    - protocol: TCP
      port: 11434
      targetPort: 11434
      name: ollama-api
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-service
  namespace: voice-agent-phase5
  labels:
    app: llm-service
    component: ai
    phase: "5"
    version: "5.0.0"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-service
      component: ai
  template:
    metadata:
      labels:
        app: llm-service
        component: ai
        phase: "5"
        version: "5.0.0"
    spec:
      initContainers:
        - name: ollama-pull-phi3
          image: ollama/ollama:latest
          command: ["/bin/sh"]
          args:
            - -c
            - |
              # Start Ollama server in background
              ollama serve &
              SERVER_PID=$!
              
              # Wait for server to be ready
              sleep 10
              
              # Pull qwen3:0.6b model (523MB)
              ollama pull qwen3:0.6b
              
              # Stop the server
              kill $SERVER_PID
          volumeMounts:
            - name: ollama-storage
              mountPath: /home/ubuntu/.ollama
          securityContext:
            runAsUser: 1000
            runAsGroup: 1000
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
              ephemeral-storage: "4Gi"
            limits:
              cpu: "1000m"
              memory: "2Gi"
              ephemeral-storage: "8Gi"
      containers:
        - name: llm-service
          image: ollama/ollama:latest
          command: ["/bin/sh"]
          args:
            - -c
            - |
              # Start Ollama server in background
              ollama serve &
              SERVER_PID=$!
              
              # Wait for server to be ready
              sleep 10
              
              # Preload the model to avoid cold start delays
              echo "Preloading qwen3:0.6b model..."
              timeout 60s ollama run qwen3:0.6b "Hello" || echo "Model preload completed (timeout or success)"
              
              # Keep the server running
              wait $SERVER_PID
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0"
            - name: OLLAMA_ORIGINS
              value: "*"
          ports:
            - containerPort: 11434
              name: ollama-api
          volumeMounts:
            - name: ollama-storage
              mountPath: /home/ubuntu/.ollama
          # Minimal resource requests for the free tier
          resources:
            requests:
              cpu: "500m"
              memory: "2Gi"
              ephemeral-storage: "4Gi"
            limits:
              cpu: "1000m"
              memory: "4Gi"
              ephemeral-storage: "8Gi"
          livenessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
            readOnlyRootFilesystem: false
            capabilities:
              drop:
                - ALL
      volumes:
        - name: ollama-storage
          emptyDir: {}
      restartPolicy: Always 