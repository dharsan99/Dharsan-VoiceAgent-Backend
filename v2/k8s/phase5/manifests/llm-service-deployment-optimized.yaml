apiVersion: v1
kind: Service
metadata:
  name: llm-service
  namespace: voice-agent-phase5
  labels:
    app: llm-service
    component: ai
    phase: "5"
    version: "5.0.1"
spec:
  type: ClusterIP
  selector:
    app: llm-service
    component: ai
  ports:
    - protocol: TCP
      port: 11434
      targetPort: 11434
      name: http
---
# Persistent Volume Claim for Ollama model storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-model-storage-pvc
  namespace: voice-agent-phase5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi  # Space for qwen3:0.6b model and future models
  storageClassName: standard-rwo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-service
  namespace: voice-agent-phase5
  labels:
    app: llm-service
    component: ai
    phase: "5"
    version: "5.0.1"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-service
      component: ai
  template:
    metadata:
      labels:
        app: llm-service
        component: ai
        phase: "5"
        version: "5.0.1"
    spec:
      initContainers:
        - name: ollama-pull-model
          image: ollama/ollama:latest
          command: ["/bin/sh"]
          args:
            - -c
            - |
              # Start Ollama server in background
              ollama serve &
              SERVER_PID=$!
              
              # Wait for server to be ready
              sleep 10
              
              # Check if model already exists in persistent storage
              if ollama list | grep -q "qwen3:0.6b"; then
                echo "Model qwen3:0.6b already exists, skipping download"
              else
                echo "Pulling qwen3:0.6b model..."
                ollama pull qwen3:0.6b
                echo "Model download completed"
              fi
              
              # Stop the server
              kill $SERVER_PID
          # OPTIMIZED INIT CONTAINER RESOURCES
          resources:
            requests:
              cpu: "200m"        # Reduced from 500m
              memory: "512Mi"    # Reduced from 1Gi
              ephemeral-storage: "2Gi"
            limits:
              cpu: "500m"        # Reduced from 1 core
              memory: "1Gi"      # Reduced from 2Gi
              ephemeral-storage: "2Gi"
          volumeMounts:
            - name: ollama-storage
              mountPath: /root/.ollama
          securityContext:
            runAsUser: 0
            runAsGroup: 0
      containers:
        - name: llm-service
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
              name: http
          command: ["/bin/sh"]
          args:
            - -c
            - |
              # Start Ollama server in background
              ollama serve &
              SERVER_PID=$!
              
              # Wait for server to be ready
              sleep 10
              
              # Preload the model to avoid cold start delays
              echo "Preloading qwen3:0.6b model..."
              timeout 60s ollama run qwen3:0.6b "Hello" || echo "Model preload completed (timeout or success)"
              
              # Keep the server running
              wait $SERVER_PID
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0"
            - name: OLLAMA_ORIGINS
              value: "*"
          # OPTIMIZED RESOURCES - Based on actual usage analysis
          resources:
            requests:
              cpu: "200m"        # Reduced from 500m (actual usage: 1m)
              memory: "512Mi"    # Reduced from 1Gi (actual usage: 54Mi)
              ephemeral-storage: "2Gi"
            limits:
              cpu: "500m"        # Reduced from 1 core
              memory: "1Gi"      # Reduced from 2Gi
              ephemeral-storage: "2Gi"
          # THREE-STAGE HEALTH PROBE SYSTEM
          startupProbe:
            httpGet:
              path: /api/tags
              port: 11434
            failureThreshold: 30
            periodSeconds: 10
            initialDelaySeconds: 120  # Longer for model loading
          readinessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            periodSeconds: 10
            failureThreshold: 3
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            periodSeconds: 30
            failureThreshold: 3
            timeoutSeconds: 10
          volumeMounts:
            - name: ollama-storage
              mountPath: /root/.ollama
          securityContext:
            runAsUser: 0
            runAsGroup: 0
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            capabilities:
              drop:
                - ALL
      volumes:
        - name: ollama-storage
          persistentVolumeClaim:
            claimName: ollama-model-storage-pvc
---
# HPA for LLM Service (NEW)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-service-hpa
  namespace: voice-agent-phase5
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-service
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30 